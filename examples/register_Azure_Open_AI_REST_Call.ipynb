{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e928632-2b60-4eea-b296-1b196a29b3a4",
   "metadata": {},
   "source": [
    "Copyright © 2024, SAS Institute Inc., Cary, NC, USA.  All Rights Reserved. SPDX-License-Identifier: Apache-2.0\n",
    "# Register Azure OpenAI GPT Model REST API Calls\n",
    "In this notebook, we will walk through the steps required for leveraging a GPT-3.5-Turbo model from Azure OpenAI in SAS® Model Manager® and SAS® Intelligent Decisioning®. \n",
    "*** \n",
    "## Table of Contents \n",
    "1. [Introduction](#Introduction)\n",
    "1. [Deploying a GPT Model in Azure OpenAI](#Deploying-a-GPT-Model-in-Azure-OpenAI)\n",
    "1. [Considerations for Key Management](#Considerations-for-Key-Management)\n",
    "1. [Integration with SAS Model Manager](#Integration-with-SAS-Model-Manager)\n",
    "1. [Integration with SAS Intelligent Decisioning](#Integration-with-SAS-Intelligent-Decisioning)\n",
    "1. [Conclusion](#Conclusion)\n",
    "***\n",
    "## Introduction \n",
    "[Azure OpenAI Service](https://azure.microsoft.com/en-us/products/ai-services/openai-service/) provides access to Large Language Models (LLMs) through REST API, their Python SDK, and a web-based interface in the Azure OpenAI studio. Various models are available out of the box, including GPT-4, GPT-4 Turbo with Vision, GPT-3.5-Turbo, and Embeddings model series. You can read more about Azure OpenAI Service [here](https://learn.microsoft.com/en-us/azure/ai-services/openai/overview). To incorporate these models in SAS Intelligent Decisioning and SAS Model Manager, we will create a Python function and scoring code calling the REST API of a GPT-3.5-Turbo model deployed in Azure OpenAI. At the time of writing this notebook, the Python SDK does not support all calls to each model type, but that may change in the future. Additionally, we will walk through the steps required for deploying the model, based on what is required at the time of writing.  \n",
    "*** \n",
    "## Deploying a GPT Model in Azure OpenAI\n",
    "At the time of writing, access to Azure OpenAI service must be requested and approved. You can determine if you qualify for access and submit your request via [this form]( https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbR7en2Ais5pxKtso_Pz4b1_xUNTZBNzRKNlVQSFhZMU9aV09EVzYxWFdORCQlQCN0PWcu). Once access is granted, you must create an Azure OpenAI Service resource and deploy your model. [This tutorial](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/create-resource?pivots=web-portal) walks through setting up an Azure OpenAI Service and deploying a model in a few simple steps. For this example, I’ve used GPT-3.5-Turbo, but not all models are available in all regions. Next, I recommend walking through [this tutorial](https://learn.microsoft.com/en-us/azure/ai-services/openai/quickstart?pivots=rest-api&tabs=command-line%2Cpython) with REST as your preferred method. Importantly, save your endpoint, API-key, and deployment name. We will need them for the next steps. Run the cell below and you can save each of them via the input boxes that appear.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2aebc5-a4c6-4048-84cc-b0d9b15e9ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "endpoint = input(\"Endpoint: \")\n",
    "deployment_name = input(\"Deployment name: \")\n",
    "api_key = getpass.getpass(\"API Key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89abd7f2-200a-4b90-a029-d87513544775",
   "metadata": {},
   "source": [
    "***\n",
    "## Considerations for Key Management\n",
    "Even though the cell above collects your API key, it uses the GetPass package to hide your key from view. With our endpoint, deployment name, and API key, anyone can make calls to our deployed LLM. Each call incurs a cost, so we need to prevent unauthorized usage. We want to take steps to protect our API key from falling into the wrong hands, but we need to use our API key in our score code to call our deployed LLM. Let’s outline a few key management options and discuss the potential risks of each. \n",
    "\n",
    "First, we can hardcode our key into the score code used in SAS Model Manager and SAS Intelligent Decisioning. This will give anyone with access to our models or Python code files on SAS Viya the ability to view our key, and thus, use our model. We can restrict access to these resources in SAS Viya by leveraging the [access control rules](https://go.documentation.sas.com/doc/en/sasadmincdc/v_049/evfun/n1uw3er96phzpfn1pxvnf01f6sw3.htm) built into SAS Viya so only a select few can see our models or Python code. \n",
    "\n",
    "We can make this approach a bit better by encoding our key using the Base64 package and then adding logic into our score code to decode our key prior to make the REST API call. Anyone can decode our encoded key, but now our key is no longer stored plain text. You can encode your key via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e99e596-b923-4c18-9a7f-6ba1132f86fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ZXhhbXBsZV9rZXk='"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import base64\n",
    "key = 'example_key'\n",
    "en = base64.b64encode(key.encode('ascii')).decode('ascii')\n",
    "en"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c645cd4-06a6-4637-bdf8-0acebba79954",
   "metadata": {},
   "source": [
    "And you can decode your key by: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbf669bd-473d-4f0b-b5f8-e3543abd8aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'example_key'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de = base64.b64decode(en.encode('ascii')).decode('ascii')\n",
    "de"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978fd715-96b1-496c-8222-c30e862edcb4",
   "metadata": {},
   "source": [
    "Anyone with access to our model, decision, or deployment can use it to make a call to our deployed LLM. If we want to restrict usage to only folks with our key, we can add our key or our encoded key as an input to the model or decision. When calling the deployed model or decision, the individual must also pass the key as an input variable. When performing score tests in CAS or publishing validation, this will write the key to the CAS table, where it may be viewable to others. Additionally, when scoring via MAS, this may write the key to the log viewable by administrators. \n",
    "\n",
    "Finally, with help from an administrator, you can save the keys as Environment variables Python environment within SAS Viya. This allows anyone with access to the model, decision, or deployment access to our deployed LLM, but this does hide our key from others. For example, in a Python environment where the model to be executed, we can save an environment variable like so: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ddd8878-ebe6-43ee-b088-035334e8dba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['my_gpt_key'] = key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c70757-bec8-49d9-ba7b-9e216c3da0f3",
   "metadata": {},
   "source": [
    "And use it our code, like so: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6b38665-2dba-49a3-98e9-535135e5c40c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'example_key'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = os.environ['my_gpt_key']\n",
    "k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59bb4f6-936e-4ebd-86de-a0fb1bdb8183",
   "metadata": {},
   "source": [
    "The key management techniques above can be combined to select the best level of security for your needs. You will need to update the code blocks below to take advantage of the techniques you’ve chosen. \n",
    "*** \n",
    "## Integration with SAS Model Manager\n",
    "To run our model in SAS Model Manager, we need to write our score code and save it as a .py file. We also need to specify the inputs to our score code, the outputs the score code generates, and properties of the model. Then we can register it all into SAS Model Manager directly from this notebook using the python-sasctl package. First, let’s use python-sasctl to authenticate to our target SAS Viya environment. Run the cell below to input your information to create a session with SAS Viya. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8366fa36-4b33-4b1b-9729-dba501cecd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sasctl import Session\n",
    "from sasctl.services import model_repository as mr, model_management as mm\n",
    "\n",
    "host = input(\"Hostname: \")\n",
    "username = input(\"Username: \")\n",
    "password = getpass.getpass(\"Password: \")\n",
    "\n",
    "sess = Session(host, username, password)\n",
    "sess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcbf808-b3d1-430f-88c5-92bc65cbd83b",
   "metadata": {},
   "source": [
    "Next, let’s write our score code. Keep in mind your chosen key management techniques and edit the score code to reflect your choice. In the block below, you need to: \n",
    "1. Pick a name for your model and update the write file line to match your chosen name. This will write the file to your current directory, but you can edit this line to save the file to another directory. \n",
    "1. Update the endpoint and deployment name within the URL string.\n",
    "1. Update the key variable to reflect your key management strategy as well as make any necessary changes within SAS Viya.\n",
    "\n",
    "Run the block to write the score code file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0563c76-91a3-4c85-87f2-d99e13144899",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile [INSERT-YOUR-MODEL-NAME].py\n",
    "import requests\n",
    "\n",
    "def score(prompt):\n",
    "    \"Output: answer_llm,finish_reason\"\n",
    "    \n",
    "    url = '[INSERT-YOUR-ENDPOINT]/openai/deployments/[INSERT-YOUR-DEPLOYMENT-NAME]/chat/completions?api-version=2023-05-15'\n",
    "\n",
    "    k =  '[INSERT-YOUR-API-KEY]'\n",
    "\t\n",
    "    h =  {\"Accept\": \"application/json\", \"Content-type\": \"application/json; charset=utf-8\", \"api-key\": k}\n",
    "\n",
    "    data = {\"messages\":[{\"role\": \"user\", \"content\": prompt}]}\n",
    "\n",
    "    response = requests.post(url, json = data , headers = h )\n",
    "\n",
    "    jsonResponse = response.json()   \n",
    "    finish_reason = jsonResponse[\"choices\"][0]['finish_reason']\n",
    "    answer_llm = jsonResponse[\"choices\"][0]['message']['content']\n",
    "\n",
    "\n",
    "    return answer_llm,finish_reason"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90363ad-b7ec-4195-97e9-e56f1e3cf92f",
   "metadata": {},
   "source": [
    "Next, let’s add our file to SAS Model Manager and update its properties. In the blow below, you need to:\n",
    "1. Add the name of your project in SAS Model Manager, your chosen name for the model, and the algorithm from your deployed LLM, such as GPT 3-5 Turbo, in the first three lines of code.\n",
    "1. Update the add_model_content function with your model name in the two indicated locations. \n",
    "Once the code block has run, you can open your project in SAS Model Manager to find your model! From SAS Model Manager, you can run the model in a score test or deploy it to other destinations, including a container. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66efa86-6e80-4f35-aee4-ebd92c55c725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update these variable to match your project\n",
    "project = '[INSERT-YOUR-PROJECT-NAME]'\n",
    "model_name = '[INSERT-YOUR-MODEL-NAME]'\n",
    "algorithm = '[INSERT-YOUR-LLM-ALGORITHM]'\n",
    "\n",
    "# Specify input variables and output variables\n",
    "inputvariables = [{'name': 'prompt', 'role': 'input', 'type': 'string', 'level': 'nominal', 'length': 500}]\n",
    "outputvariables = [{'name': 'answer_llm', 'role': 'output', 'type': 'string', 'level': 'nominal', 'length': 500}, {'name': 'finish_reason', 'role': 'output', 'type': 'string', 'level': 'nominal', 'length': 15}]\n",
    "\n",
    "# Create the model\n",
    "model = mr.create_model(\n",
    "    model=model_name,\n",
    "    project=project,\n",
    "    algorithm=algorithm,\n",
    "    modeler=username,\n",
    "    tool='Python 3',\n",
    "    function = \"Text Generation\",\n",
    "    score_code_type = 'Python',\n",
    "    input_variables = inputvariables,\n",
    "    output_variables = outputvariables\n",
    ")\n",
    "\n",
    "# Add score code\n",
    "scorefile = mr.add_model_content(\n",
    "    model,\n",
    "    open('[INSERT-YOUR-MODEL-NAME].py', 'rb'),\n",
    "    name='[INSERT-YOUR-MODEL-NAME].py',\n",
    "    role='score'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c758627-6995-4184-af93-966a8c6eacb2",
   "metadata": {},
   "source": [
    "***\n",
    "## Integration with SAS Intelligent Decisioning \n",
    "We can use the Python model we just developed and registered to SAS Model Manager within our decision flow as a model. But, if you don’t have SAS Model Manager, no need to worry! We can leverage a Python code file in SAS Intelligent Decisioning instead. To run the GPT-3.5 model, we need to create an execute function that we will then copy and paste the code into the Python code files in SAS Intelligent Decisioning. The same concerns apply for key management, so update the code block below to reflect your key management strategy. In the code block below, you need to: \n",
    "1. Update the endpoint and deployment name within the URL string. \n",
    "1. Update the key variable to reflect your key management strategy as well as make any necessary changes within SAS Viya. \n",
    "1. Copy the code block into a Python code file in SAS Intelligent Decisioning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13e819d-4ba8-4174-b7ed-f9df56af8e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' List all output parameters as comma-separated values in the \"Output:\" docString. Do not specify \"None\" if there is no output parameter. '''\n",
    "''' List all Python packages that are not built-in packages in the \"DependentPackages:\" docString. Separate the package names with commas on a single line. '''\n",
    "\n",
    "\n",
    "import requests\n",
    "\n",
    "def execute (prompt):\n",
    "   'Output:answer_llm,finish_reason'\n",
    "   'DependentPackages: requests'\n",
    "   \n",
    "   url = '[INSERT-YOUR-ENDPOINT]/openai/deployments/[INSERT-YOUR-DEPLOYMENT-NAME]/chat/completions?api-version=2023-05-15'\n",
    "\n",
    "   k =  '[INSERT-YOUR-API-KEY]'\n",
    "\t\n",
    "   h =  {\"Accept\": \"application/json\", \"Content-type\": \"application/json; charset=utf-8\", \"api-key\": k}\n",
    "\n",
    "   data = {\"messages\":[{\"role\": \"user\", \"content\": prompt}]}\n",
    "\n",
    "   response = requests.post(url, json = data , headers = h )\n",
    "\n",
    "   jsonResponse = response.json()   \n",
    "   finish_reason = jsonResponse[\"choices\"][0]['finish_reason']\n",
    "   answer_llm = jsonResponse[\"choices\"][0]['message']['content']\n",
    "\n",
    "\n",
    "   return answer_llm,finish_reason\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6640cc88-94ac-4cb8-926c-715045635c08",
   "metadata": {},
   "source": [
    "Once you add those changes to your code block above, it becomes a valid Python function and you can run here using the execute function to test it, like so: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b09883-9c08-4647-a7f3-3de062fcf212",
   "metadata": {},
   "outputs": [],
   "source": [
    "execute(\"Write a tagline for an ice cream shop.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0be03a-3765-4649-8a69-9decaee5a55b",
   "metadata": {},
   "source": [
    "*** \n",
    "## Conclusion\n",
    "Now, you are all set to leverage a GPT-3.5-Turbo model deployed in Azure OpenAI in SAS Model Manager or SAS Intelligent Decisioning where it can be managed with other models in your organizations, combined with business logic, orchestrated with other models, and deployed into destinations within SAS Viya or beyond using containers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31afee6d-b46b-4560-9c8a-33bcb41f2e93",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
